{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate your AI development with Pre-Trained Models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "To run this notebook, you will need the following:\n",
    "- Python 3.5/3.6 (via Anaconda)\n",
    "- Tensorflow > 1.2\n",
    "- Keras\n",
    "\n",
    "Image processing tasks will be much faster with a GPU and GPU enabled tensorflow.\n",
    "\n",
    "To install tensorflow_gpu, follow the instructions at: [tf_GPU link](https://www.tensorflow.org/install/install_linux)\n",
    "\n",
    "If you do not have access to a local GPU, you can get access to a GPU enabled VM through [AWS](https://aws.amazon.com/), [Azure](portal.azure.com), or services such as [Paperspace](paperspace.com). \n",
    "\n",
    "[Google CoLab](https://colab.research.google.com) will also give you access to a K80 accelerator.\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "dogscats is available from fastai [download](http://files.fast.ai/data/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks in 5(ish) minutes\n",
    "\n",
    "For more detailed explanations, I recomend Brandon Roeher's materials on [artificial neural networks](Link.com) and [convolutional neural networks](link.com).\n",
    "\n",
    "\n",
    "#### Weights, Activations, Backpropigation\n",
    "\n",
    "For starters: matrix multiplication, dot products, and activation functions. \n",
    "\n",
    "In a _linear regression_ with two inputs (x_1, x_2), we could represent a model as y = w_1*x_1 + w_2*x_2\n",
    "In a _logistic regression_, we would take our same inputs, multiply each of them by a weight, and transform them with a logit function - y = 1/(1+e^-(w_1*x_1 + w_2*x_2))\n",
    "\n",
    "\n",
    "![Ann](./images/ann.png)\n",
    "\n",
    "In a neural network, we perform a similar action - at every node. We take an input vector x (x_1...x_n), take the dot product of our input vector and a weight vector, and then transform the output with a function. A simple network with 3 inputs (X1,X2,X3), and 2 neurons (called perceptrons) using a sigmoid activation would have 6 weights (2 sets of 3 weights), and calculate 2 sigmoid functions.\n",
    "\n",
    "![Ann2](./images/ann2.gif)\n",
    "\n",
    "While sigmoid activations are the traditional example (tanh, softplus, etc), linear activations such as Rectified Linear Units (ReLu) have become popular due to their advantages in training.\n",
    "\n",
    "This is the forward pass - in order to optimize and train the network, we propagate the error (difference between our calculated value and the intended \"true\" value) backwards through the network. **Very** loosely - if we think of each layer as being dependent on previous layers, we can adjust the weights at each layer with respect to weights further in the network by taking the partial dervative of the activation at each layer and adjusting the weight with respect to the gradient (_descending the gradient_).\n",
    "\n",
    "[Andrew Ng on backprop](https://www.youtube.com/watch?v=mOmkv5SI9hU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "For computer vision and image based tasks, we are often dealing with CNNs, or convolutional networks. In a CNN, there are two important layers: convolutional layers (passing kernels over an image) and pooling layers (summarizing layers, usually taking the maximum, to down sample)\n",
    "\n",
    "Convolution:\n",
    "![CNN](./images/conv.png)\n",
    "\n",
    "Convolution Process:\n",
    "![CNN](./images/conv.gif)\n",
    "\n",
    "Pooling:\n",
    "![Pool](./images/pool.png)\n",
    "\n",
    "In practice, we use banks of filters - at each block of convolutional layers, there may be 100's of 3x3 filters. In a traditional MLP network, we are training the weights. In a convolutional network, we are training the kernels.\n",
    "\n",
    "[Stanford Lecture on CNN](https://www.youtube.com/watch?v=AQirPKrAyDg)\n",
    "\n",
    "[Stanford Slides on CNN Architectures](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture - VGG\n",
    "\n",
    "[Original Paper - Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG Arch](./images/vgg16_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG Table](./images/vgg16_table.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import ResNet50, VGG16\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = VGG16(weights='imagenet', include_top=True)\n",
    "m.compile(optimizer='rmsprop', \n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture - VGG\n",
    "\n",
    "[Original Paper - Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385v1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Residual Learning Block](./images/ResNet.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNetScroll](./images/ResNet.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ResNet50(weights='imagenet', include_top=True)\n",
    "m.compile(optimizer='rmsprop', \n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training\n",
    "\n",
    "VGG and ResNet were both designed for the [ImageNet](http://www.image-net.org/) image classification dataset - with the goal of achieving the best possible top-5 likelihood score for 1000 possible classes for a given image. The networks were trained for days on GPUs to achieve their optimal weights, and one can imagine that the over-all training time during development can be accurately measured in weeks.\n",
    "\n",
    "### **Our problem: We don't have days and weeks to spend training networks (assuming our architecture works)**\n",
    "\n",
    "What we can do, is jump-start our process by using the layers and pre-trained weights from proven architectures like VGG or ResNet, and either adding new layers which we train on our new training data, or selectively training layers in the existing architectures to adjust them to our specific needs. In this fasion, we can cut the training time needed to produce a production ready architecture down from days to minutes or hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets give it a run: Dogs and Cats\n",
    "\n",
    "In this example, we'll use VGG16 with weights pre-trained on the imagenet dataset to jump-start our training process. We can selectively un-freeze layers to adjust how much we're customizing the network to our needs, or add new layers that we'll train to our specific problem - identifying cats and dogs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in os.listdir('../data/dogscats/train/cats/')[:3]:\n",
    "    path = '../data/dogscats/train/cats/' + cat\n",
    "    im = Image.open(path).resize((244,244))\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dog in os.listdir('../data/dogscats/train/dogs/')[:3]:\n",
    "    path = '../data/dogscats/train/dogs/' + dog\n",
    "    im = Image.open(path).resize((244,244))\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Keras \n",
    "\n",
    "\"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\"\n",
    "\n",
    "[Keras Website & Documentation](https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#numpy\n",
    "import numpy as np\n",
    "\n",
    "#OpenCV\n",
    "import cv2\n",
    "\n",
    "#Keras Processing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "#Keras Models & Layers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "#Keras - Pretrained Models\n",
    "from keras.applications import resnet50,vgg16,VGG16,ResNet50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = '../data/dogscats/train/'\n",
    "validation_path = '../data/dogscats/valid/'\n",
    "test_path= '../data/dogscats/sample/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#image size\n",
    "sz = 244\n",
    "\n",
    "#batch size\n",
    "bz = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "    shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "print('Train:')\n",
    "train_generator = train_datagen.flow_from_directory(train_path,\n",
    "                                                    shuffle=True,\n",
    "    target_size=(sz, sz),\n",
    "    batch_size=bz, class_mode='binary')\n",
    "\n",
    "print('Validation:')\n",
    "validation_generator = test_datagen.flow_from_directory(validation_path,\n",
    "    shuffle=False,\n",
    "    target_size=(sz, sz),\n",
    "    batch_size=bz, class_mode='binary')\n",
    "\n",
    "print('Test Sample:')\n",
    "test_generator = test_datagen.flow_from_directory(test_path,\n",
    "    shuffle=False,\n",
    "    target_size=(sz, sz),\n",
    "    batch_size=1, class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model1 - VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG](./images/vgg16.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Base Model - VGG\n",
    "base_model_vgg = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "#Add new layers\n",
    "vgg_new = base_model_vgg.output\n",
    "vgg_new = GlobalAveragePooling2D()(vgg_new)\n",
    "vgg_new = Dense(512, activation='relu')(vgg_new)\n",
    "\n",
    "#Make Predictions\n",
    "predictions_vgg = Dense(1, activation='softmax')(vgg_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_vgg = Model(inputs=base_model_vgg.input, outputs=predictions_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model_vgg.layers: \n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_vgg.compile(optimizer='rmsprop', \n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.fit_generator(train_generator, train_generator.n // bz, epochs=1, workers=3,\n",
    "        validation_data=validation_generator, validation_steps=validation_generator.n // bz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_preds = 8\n",
    "preds = model_vgg.predict_generator(test_generator,n_preds)\n",
    "\n",
    "for i in range(0,n_preds):\n",
    "    display(Image.open(test_path+'/'+test_generator.filenames[i]).resize((244,244)))\n",
    "    print(float(\"{:.4f}\".format(float(preds[i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train some conv. layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model_vgg.layers[:15]: layer.trainable = False\n",
    "for layer in model_vgg.layers[15:]: layer.trainable = True\n",
    "model_vgg.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_vgg.fit_generator(train_generator, train_generator.n // bz, epochs=3, workers=3,\n",
    "        validation_data=validation_generator, validation_steps=validation_generator.n // bz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_preds = 8\n",
    "preds = model_vgg.predict_generator(test_generator,n_preds)\n",
    "\n",
    "for i in range(0,n_preds):\n",
    "    display(Image.open(test_path+'/'+test_generator.filenames[i]).resize((244,244)))\n",
    "    print(float(\"{:.4f}\".format(float(preds[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model2 - ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "for layer in base_model.layers: layer.trainable = False\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, train_generator.n // bz, epochs=3, workers=4,\n",
    "        validation_data=validation_generator, validation_steps=validation_generator.n // bz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_preds = 8\n",
    "preds = model.predict_generator(test_generator,n_preds)\n",
    "\n",
    "for i in range(0,n_preds):\n",
    "    display(Image.open(test_path+'/'+test_generator.filenames[i]).resize((244,244)))\n",
    "    print(float(\"{:.4f}\".format(float(preds[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_at = 140\n",
    "for layer in model.layers[:split_at]: layer.trainable = False\n",
    "for layer in model.layers[split_at:]: layer.trainable = True\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[140:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, train_generator.n // bz, epochs=1, workers=3,\n",
    "        validation_data=validation_generator, validation_steps=validation_generator.n // bz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_preds = 8\n",
    "preds = model.predict_generator(test_generator,n_preds)\n",
    "\n",
    "for i in range(0,n_preds):\n",
    "    display(Image.open(test_path+'/'+test_generator.filenames[i]).resize((244,244)))\n",
    "    print(float(\"{:.4f}\".format(float(preds[i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Networks - Uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above demos we've used VGG16 and ResNet50 architectures to classify new images that they did not see in their full training sequence. \n",
    "\n",
    "**What else can we use pre-trained networks for?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained networks work best in domains where inputs are relatively similar across implementations. \n",
    "\n",
    "## Image Classication\n",
    "\n",
    "As we've seen above, CNN architectures can be used to jump start new image classification tasks.\n",
    "\n",
    "In fact, a very influential architecture called R-CNN (followed/associated with Fast R-CNN and Faster R-CNN) use pre-trained VGG or ResNet networks in their process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-CNN\n",
    "\n",
    "![RCNN](./images/rcnn.png)\n",
    "\n",
    "\n",
    "#### Fast R-CNN\n",
    "![FRCNN](./images/FastRCNN.png)\n",
    "\n",
    "[Faster R-CNN Video](https://www.youtube.com/watch?v=WZmSMkK9VuA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Style transfer gets good press - and commonly uses VGGnet / AlexNet architectures to produce representations that can be transfered onto a new image\n",
    "\n",
    "![Neural Style](./images/style.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not as numerous as image processing networks, there are some famous text processing networks that can be used with pre-trained weights. They employ RNN and LSTM architectures to handle tasks like predicting the next word (think autocomplete) or even the next character in a sequence. Or answering questions after parsing a body of text.\n",
    "\n",
    "More likely you'll find a pre-set architecture you can use and a provided training corpus than a set of pre-trained weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to Consider:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work with VGG or ResNet for a while and you'll quickly realize that they work best when using images that roughly resemble their training materials (seems obvious, I know). ResNet trained on ImageNet does best when looking at font-on or side-on pictures of objects.\n",
    "\n",
    "This really comes into play with pre-trained networks for text. Many networks are trained on standard data sets, which are compiled from different types of sources. A model trained on news articles is going to produce different results than the same model trained on Yelp reviews or wikipedia articles.\n",
    "\n",
    "For example, in text models words are often transitioned to vectors using GloVe embeddings - training your own embeddings requires gathering large volumes of data and training a network for a long period of time. For most use cases it is faster and just as accurate to convert your text using pre-trained embeddings. \n",
    "\n",
    "[Stanford offers several versions for download](https://nlp.stanford.edu/projects/glove/) some of which are several GB, compressed:\n",
    "- Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors)\n",
    "- Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors)\n",
    "- Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors)\n",
    "- Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors)\n",
    "\n",
    "One can imagine getting very different results using a model built on repressentations of words as they exist on Wikipedia, compared to the same word's usage on Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
