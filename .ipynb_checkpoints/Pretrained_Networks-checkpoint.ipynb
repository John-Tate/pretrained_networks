{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate your AI journey with Pre-Trained Models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "To run this notebook, you will need the following:\n",
    "- Python 3.5/3.6 (Anaconda)\n",
    "- Tensorflow > 1.4\n",
    "- Keras\n",
    "- cython\n",
    "- opencv\n",
    "- easdict\n",
    "\n",
    "Image processing tasks will be much faster with a GPU and GPU enabled tensorflow.\n",
    "\n",
    "To install tensorflow_gpu, follow the instructions at: [tf_GPU link](link.com)\n",
    "\n",
    "If you do not have access to a local GPU, you can get access to a GPU enabled VM through [AWS](aws.com), [Azure](portal.azure.com), or services such as [Paperspace](paperspace.com). \n",
    "\n",
    "[Google CoLab](google.com) will also give you access to a K80 accelerator.\n",
    "\n",
    "\n",
    "### Acknowledgements / Thefts\n",
    "\n",
    "Most of the code below is not my own. Cats & Dogs dataset stolen from [fast.ai](fast.ai), Faster R-CNN implemenation stolen from [smallcorgi](smallcorgi.githum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks in 5 minutes\n",
    "\n",
    "For more detailed explanations, I recomend Brandon Roeher's materials on [artificial neural networks](Link.com) and [convolutional neural networks](link.com).\n",
    "\n",
    "\n",
    "### How do they work\n",
    "\n",
    "#### Activation Functions\n",
    "#### Weights\n",
    "#### SGD\n",
    "#### Backpropigation\n",
    "#### Convolutions\n",
    "\n",
    "### Architecture\n",
    "\n",
    "[VGG Arch]\n",
    "\n",
    "[ResNet Arch]\n",
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "VGG and ResNet were both designed for the [ImageNet](imagenet.com) image classification dataset - with the goal of achieving the best possible top-5 likelihood score for 1000 possible classes for a given image. The networks were trained for days on GPUs to achieve their optimal weights, and one can imagine that the over-all training time during development can be accurately measured in weeks.\n",
    "\n",
    "**Our problem: We don't have days and weeks to spend training networks (assuming our architecture works)**\n",
    "\n",
    "What we can do, is jump-start our process by using the layers and pre-trained weights from proven architectures like VGG or ResNet, and either adding new layers which we train on our new training data, or selectively training layers in the existing architectures to adjust them to our specific needs. In this fasion, we can cut the training time needed to produce a production ready architecture down from days to minutes or hours.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Cats, Dogs & VGG\n",
    "\n",
    "In this example, we'll use VGG16 with weights pre-trained on the imagenet dataset to jump-start our training process. We can selectively un-freeze layers to adjust how much we're customizing the network to our needs, or add new layers that we'll train to our specific problem - identifying cats and dogs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
